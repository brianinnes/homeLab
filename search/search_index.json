{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This site will walk you through how to create your own home or office Lab environment, enabling you to explore Red Hat OpenShift, using oVirt and OKD. This setup uses the simplest possible network configuration with a single physical workstation hosting the entire cluster. There are other online resources which explain how to set up a multi-machine hosted installation with more complex network setup. I've included a few links in the resources section of this site.","title":"Home"},{"location":"my-setup/","text":"My setup \u00b6 This page details the setup for my home lab Hardware \u00b6 There are 2 systems the make up the lab hardware: Raspberry Pi 4 for network services Refurbished HP Z840 Workstation for the main Lab system Raspberry Pi 4 \u00b6 System : Raspberry Pi 4 Memory : 8GB Memory Storage : 1TB SSD external disk connected via USB 3 The Raspberry Pi provides the core network services needed for the Home lab. Many Internet Service Providers provide a home hub that will suffice for most home environments, but my hub doesn't provide the DHCP and DNS services I need to run my home lab, so I disabled the basic DHCP service offered by my ISP hub and setup a Raspberry Pi single board computer to provide the network services for my home network. Dnsmasq provides both DHCP and DNS services. It is easy to setup and ideal for a home lab. I run Dnsmasq on a raspberry Pi as part of my home network setup. HP Z840 Workstation \u00b6 System : Refurbished Z840 Workstation CPU : 2 X Intel Xeon E5-2697 V4 2.30Ghz 18 Core CPUs Memory : 384GB Storage : 2TB SSD NVME M.2 - Boot disk 4 x 6TB SATA 7200rpm Disks configured as 10.9 TB LSI RAID 10 volume 2 X 2TB SATA SSDs configured as 3.64 TB LSI RAID 0 volume The z840 is setup as a single node oVirt and Gluster Hyperconverged system. The LSI RAID volumes are used exclusively as gluster volumes, so oVirt and OKD have access to both fast SSD and slower HD storage. The systems are connected with 1GB networking using unmanaged switches and cat 6 cables. Network setup \u00b6 I run the lab environment on my home ethernet network, which is not exposed on the internet, so all the name resolution is entirely within my local LAN, so I've not purchased any domain for the home lab. Base domain name : lab.home OKD Cluster name : okd Network IP Network : 192.168.1.x Netmask : 24 / 255.255.255.0 Gateway : 192.168.1.1 DNS : 192.168.1.10 DHCP range : 192.168.1.100 - 192.168.1.249 OKD Cluster name : okd DNS mappings on local network \u00b6 IP Hostnames Description 192.168.1.10 raspihome raspihome.lab.home Raspberry pi host 192.168.1.11 z840 z840.lab.home z840 host 192.168.1.15 virt virt.lab.home oVirt hosted engine 192.168.1.16 workstation workstation.lab.home Fedora workstation 192.168.1.17 api api.okd.lab.home OKD api endpoint 192.168.1.18 *.apps.okd.lab.home OKD apps ingress endpoint","title":"My setup"},{"location":"my-setup/#my-setup","text":"This page details the setup for my home lab","title":"My setup"},{"location":"my-setup/#hardware","text":"There are 2 systems the make up the lab hardware: Raspberry Pi 4 for network services Refurbished HP Z840 Workstation for the main Lab system","title":"Hardware"},{"location":"my-setup/#raspberry-pi-4","text":"System : Raspberry Pi 4 Memory : 8GB Memory Storage : 1TB SSD external disk connected via USB 3 The Raspberry Pi provides the core network services needed for the Home lab. Many Internet Service Providers provide a home hub that will suffice for most home environments, but my hub doesn't provide the DHCP and DNS services I need to run my home lab, so I disabled the basic DHCP service offered by my ISP hub and setup a Raspberry Pi single board computer to provide the network services for my home network. Dnsmasq provides both DHCP and DNS services. It is easy to setup and ideal for a home lab. I run Dnsmasq on a raspberry Pi as part of my home network setup.","title":"Raspberry Pi 4"},{"location":"my-setup/#hp-z840-workstation","text":"System : Refurbished Z840 Workstation CPU : 2 X Intel Xeon E5-2697 V4 2.30Ghz 18 Core CPUs Memory : 384GB Storage : 2TB SSD NVME M.2 - Boot disk 4 x 6TB SATA 7200rpm Disks configured as 10.9 TB LSI RAID 10 volume 2 X 2TB SATA SSDs configured as 3.64 TB LSI RAID 0 volume The z840 is setup as a single node oVirt and Gluster Hyperconverged system. The LSI RAID volumes are used exclusively as gluster volumes, so oVirt and OKD have access to both fast SSD and slower HD storage. The systems are connected with 1GB networking using unmanaged switches and cat 6 cables.","title":"HP Z840 Workstation"},{"location":"my-setup/#network-setup","text":"I run the lab environment on my home ethernet network, which is not exposed on the internet, so all the name resolution is entirely within my local LAN, so I've not purchased any domain for the home lab. Base domain name : lab.home OKD Cluster name : okd Network IP Network : 192.168.1.x Netmask : 24 / 255.255.255.0 Gateway : 192.168.1.1 DNS : 192.168.1.10 DHCP range : 192.168.1.100 - 192.168.1.249 OKD Cluster name : okd","title":"Network setup"},{"location":"my-setup/#dns-mappings-on-local-network","text":"IP Hostnames Description 192.168.1.10 raspihome raspihome.lab.home Raspberry pi host 192.168.1.11 z840 z840.lab.home z840 host 192.168.1.15 virt virt.lab.home oVirt hosted engine 192.168.1.16 workstation workstation.lab.home Fedora workstation 192.168.1.17 api api.okd.lab.home OKD api endpoint 192.168.1.18 *.apps.okd.lab.home OKD apps ingress endpoint","title":"DNS mappings on local network"},{"location":"okd_install/","text":"OKD Install \u00b6 When you have your oVirt environment up and running you can install OKD. Preparing required information \u00b6 To help with the installation here is a list of information you will need during the install: item description Engine FQDN This is the Fully Qualified Domain Name of the virtual engine. This is the value you entered in step 1 in oVirt setup section Install the Hosted Engine Engine user This should be admin Engine password This is the value entered at ovirt install time, and the password you use to access the ovirt admin console oVirt Cluster name Should be Default . This can be verified in the oVirt Administration console, select Compute from the side menu then Clusters . By default there is a single cluster defined with Default as the name Storage Domain Select the storage domain with fast storage, ideally Solid State Disk (SSD) should be used Network Select the oVirt network to place the OKD cluster hosts on. By default a single network named ovirtmgmt is created at oVirt install IP address for the API endpoint for the OKD cluster The IP address on your network you previously allocated for the OKD cluster API Endpoint IP address for the Ingress for the OKD cluster The IP address on your network you previously allocated for the OKD Cluster Ingress OKD Cluster base domain The base domain your cluster will use for all exposed endpoints - This cannot be changed after installation Cluster Name The name for the OKD cluster, this will be prepended to the base domain to form the URLs for application and API endpoints. (e.g. api.<cluster name>.<base domain> ) Enter a valid Red Hat pull secret This is the pull secret to the Red Hat image registries where Open Shift images are stored. You don't need to use a valid pull secret as OKD does not need access to the registries, but providing a valid secret will allow Red Hat applications to be installed on the cluster. The pull secret must pass a validation check. A valid fake secret is {\"auths\":{\"fake\":{\"auth\":\"aWQ6cGFzcwo=\"}}} . If you have a Red Hat account a valid secret can be downloaded from here . Adding a valid pull secret will allow access to licensed Red Hat content, you need to ensure you comply with Red Hat entitlements if providing a valid secret Todo Add details of the required DNS config needed to allow the OKD setup to work Install OKD \u00b6 Complete the following steps to install OKD on your cluster Download the installer \u00b6 To find the latest version of the OKD installer, visit the releases page and download the latest openshift-install image for the platform where you plan to run the installer from (this is typically your local laptop, but could be a virtual machine you run on the ovirt cluster). Alternatively you can download one of the nightly builds . These builds contain recent updates and fixes, but aren't an official release, so may contain unresolved issues. Click on a build to get information about the build and also the command to download and unpack the build to your local system Expand the downloaded archive to allow access to the openshift-install binary, e.g. tar zxvf openshift-install-mac-4.9.0-0.okd-2021-08-03-074139.tar.gz Create the install config \u00b6 Open a command or terminal window and switch to the directory containing the openshift-install binary run command ./openshift-install create install-config , and enter the values when prompted - these are the values you prepared at the top of this page. select oVirt as the Platform enter the engine F ully Q ualified D omain N ame accept the suggested engine username (admin@internal) by pressing enter enter the engine password accept the default Cluster name, which should be set as Default unless you have changed oVirt configuration select the appropriate storage domain to store the OKD cluster host disk images select the Network to use input IP address allocated to OKD cluster API endpoint input IP address allocated to OKD cluster Ingress input the base domain input the cluster name (forms the end of all URLs for cluster apps apps.<cluster name>.<basse domain> ) input the pull secret (either a valid Red Hat or the fake one provided above) Warning There is a bug in the OVNKubernetes network provider in OKD 4.7. This prevents the install working without manual intervention and prevents the detection of external IP addresses causing the worker Machines being stuck in Provisioning phase. A work around is to use the OpenShiftSDN network provider. To do this change the install-config.yaml file created in the previous steps and change the networkType property from OVNKubernetes to OpenShiftSDN networking : clusterNetwork : - cidr : 10.128.0.0/14 hostPrefix : 23 machineNetwork : - cidr : 10.0.0.0/16 networkType : OpenShiftSDN serviceNetwork : - 172.30.0.0/16 Warning From OKD 4.8 the default install configuration now defines affinity groups. If you do not have at least 3 physical oVirt hosts then you need to alter the install configuration to not enforce the affinity groups. To do this in the platform section at the bottom of the install-config.yaml file, set the enforcing property to false for the affinity groups: platform : ovirt : affinityGroups : - description : AffinityGroup for spreading each compute machine to a different host enforcing : false name : compute priority : 3 - description : AffinityGroup for spreading each control plane machine to a different host enforcing : false name : controlplane priority : 5 Run the installer \u00b6 Note The default install will download the required content from the internet. It is assumed that the machine running the openshift-install application and the machine hosting oVirt and the OKD cluster have a good internet connection to be able to download the required content. create an install directory mkdir install copy the configuration into the install directory - don't move it, copy it as it gets deleted during the install, so it is good to have a backup copy. cp install-config.yaml install create the cluster with command ./openshift-install create cluster --dir=./install --log-level=info wait until the setup completes, this can take quite a while, 30-60 minutes is not uncommon when the install completes you will see a message similar to this: INFO Install complete! INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/Users/brian/projects/okd/4.7/install/auth/kubeconfig' INFO Access the OpenShift web-console here: https://console-openshift-console.apps.okd.lab.home INFO Login to the console with user: \"kubeadmin\", and password: \"xxxxx-xxxxx-xxxxx-xxxxx\" Warning It is important that you keep the install directory after the installation has finished, as if you ever shut your cluster down for a length of time, then the certificates used within the system may expire whilst the system is down. The only way to access the cluster to renew the certificates is with the auth credentials contained within the install directory. You also need the install directory if you want to uninstall the cluster. Info If you have a slower network connection it is possible that the install will timeout. You can continue the install process if the install times out. Depending on the place where it timeouts you can continue: Timeout during bootstrap : ./openshift-install wait-for bootstrap-complete once bootstrap completes run command : ./openshift-install destroy bootstrap to clean up the bootstrap vm the run the command to wait for the remaining install ./openshift-install wait-for install-complete Timeout after bootstrap completes, waiting for cluster to initialize run command : ./openshift-install wait-for install-complete Connecting to the cluster \u00b6 Now your cluster has been installed you want to be able to access it. There are 2 ways to access it: Web Console Command line Web UI \u00b6 After installation your cluster web console should be accessible at the URL displayed when the openshift-install command completes. The URL is of the form https://console-openshift-console.apps.<cluster name>.<base domain> . You can use the kubeadmin command, with the password displayed when the openshift-install command completed. This is the temporary cluster admin account. You should add an identity provider and create your own admin account. Instructions to complete this task are provided below. Command lined \u00b6 You can access the cluster from the command line using the oc command line tool. Please ensure the version of OC is up to date. The correct version of oc for your cluster can be downloaded from the web console. Select the question mark (?) icon next to your user name in the top menu, then select Command line tools from the menu. Then you can select the appropriate binary version of the tool for your operating system. Once you have the oc tool install you can connect to the cluster. There are 2 ways to connect to the cluster: Connecting using the kubeconfig file \u00b6 During installation a kubeconfig file was created. This content needs to be kept safe, as it can be the only way o log in if the cluster has been shutdown for a while and certificates have expired. It also provides an admin login. To use the file set the environment variable KUBECONFIG to point to the config file. The location was provided as part of the openshift-install completion message, such as : export KUBECONFIG = /Users/brian/projects/okd/4.7/install/auth/kubeconfig you can now use the oc command and be connected as an administrator. Connect as a specific user \u00b6 You can specify a user on the command line. Ensure the KUBECONFIG environment variable is not set, then login using: oc login -u <user> --server = https://api.<cluster name>.<base domain>:6443 If you are logged into the web console as the user you wish to use on the command line, then you can get the login command using the drop down menu shown when you click your username in the top menu. Select Copy login command from the menu, you will be prompted for your password, then you can select Display Token to see a login command. This uses a generated token rather than your username and password. Post install tasks \u00b6 Info If you are getting warnings that your cluster is unable to retrieve available updates because https://origin-release.svc.ci.openshift.org host cannot be found, then you have an out of date URL in the update service. You can fix this by running the following command as an admin user on the command line: oc patch clusterversion/version --patch '{\"spec\":{\"upstream\":\"https://amd64.origin.releases.ci.openshift.org/graph\"}}' --type=merge Adding an Identity provider \u00b6 You need a system to authenticate users to the system. OKD supports a number of different options for authenticating users. Here we will use the basic htpasswd authentication method, which is a simple list of users with encrypted passwords, held in a Kubernetes secrets object. Details of other authentication methods can be found in the OKD documentation Full instructions for the HTPasswd identity provider can be found in the OKD documentation Creating the users and passwords \u00b6 You need access to the htpasswd utility. This comes with most Linux distributions and MacOS. Windows users may need to get it from an apache HTTP server distribution. Create the user file and add the first user with command: htpasswd -c -B -b users.htpasswd <user_name> <password> the file users.htpasswd will be created in the current directory. Add additional users with command (this command expects to find file users.htpasswd in the current working directory): htpasswd -B -b users.htpasswd <user_name> <password> Creating the secret from the users file \u00b6 Once you have the users.htpasswd file created and populated with all the users you want on to be able to access the system, then you need to create a secret from the file with command (this command expects to find the users.htpasswd file in the current working directory): oc create secret generic htpass-secret --from-file = htpasswd = users.htpasswd -n openshift-config Creating identity provider configuration \u00b6 You can now create the identity provider using command: Linux & Mac OS cat <<EOF | kubectl apply -f - apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: my_htpasswd_provider mappingMethod: claim type: HTPasswd htpasswd: fileData: name: htpass-secret EOF Windows Todo Add Windows equiv of the command Creating an administrator account \u00b6 After adding the identity provider, you want to make one of the accounts added to the identity provider an administrator user. Before proceeding you should log into the cluster from the Web Console, using the user account you want to make an admin user. On the command line you can make the user an admin user with the following command (replacing with a valid user name created for your configured identity provider): oc adm policy add-cluster-role-to-user cluster-admin <user> Remove the kubeadmin user \u00b6 The kubeadmin user is created during installation and should only be thought of as a temporary users. Once you have configured an identity provider and made at least one of the users an administrator on the cluster then the kubeadmin user account can be removed. Use the following command to remove the user: oc delete secrets kubeadmin -n kube-system","title":"OKD install"},{"location":"okd_install/#okd-install","text":"When you have your oVirt environment up and running you can install OKD.","title":"OKD Install"},{"location":"okd_install/#preparing-required-information","text":"To help with the installation here is a list of information you will need during the install: item description Engine FQDN This is the Fully Qualified Domain Name of the virtual engine. This is the value you entered in step 1 in oVirt setup section Install the Hosted Engine Engine user This should be admin Engine password This is the value entered at ovirt install time, and the password you use to access the ovirt admin console oVirt Cluster name Should be Default . This can be verified in the oVirt Administration console, select Compute from the side menu then Clusters . By default there is a single cluster defined with Default as the name Storage Domain Select the storage domain with fast storage, ideally Solid State Disk (SSD) should be used Network Select the oVirt network to place the OKD cluster hosts on. By default a single network named ovirtmgmt is created at oVirt install IP address for the API endpoint for the OKD cluster The IP address on your network you previously allocated for the OKD cluster API Endpoint IP address for the Ingress for the OKD cluster The IP address on your network you previously allocated for the OKD Cluster Ingress OKD Cluster base domain The base domain your cluster will use for all exposed endpoints - This cannot be changed after installation Cluster Name The name for the OKD cluster, this will be prepended to the base domain to form the URLs for application and API endpoints. (e.g. api.<cluster name>.<base domain> ) Enter a valid Red Hat pull secret This is the pull secret to the Red Hat image registries where Open Shift images are stored. You don't need to use a valid pull secret as OKD does not need access to the registries, but providing a valid secret will allow Red Hat applications to be installed on the cluster. The pull secret must pass a validation check. A valid fake secret is {\"auths\":{\"fake\":{\"auth\":\"aWQ6cGFzcwo=\"}}} . If you have a Red Hat account a valid secret can be downloaded from here . Adding a valid pull secret will allow access to licensed Red Hat content, you need to ensure you comply with Red Hat entitlements if providing a valid secret Todo Add details of the required DNS config needed to allow the OKD setup to work","title":"Preparing required information"},{"location":"okd_install/#install-okd","text":"Complete the following steps to install OKD on your cluster","title":"Install OKD"},{"location":"okd_install/#download-the-installer","text":"To find the latest version of the OKD installer, visit the releases page and download the latest openshift-install image for the platform where you plan to run the installer from (this is typically your local laptop, but could be a virtual machine you run on the ovirt cluster). Alternatively you can download one of the nightly builds . These builds contain recent updates and fixes, but aren't an official release, so may contain unresolved issues. Click on a build to get information about the build and also the command to download and unpack the build to your local system Expand the downloaded archive to allow access to the openshift-install binary, e.g. tar zxvf openshift-install-mac-4.9.0-0.okd-2021-08-03-074139.tar.gz","title":"Download the installer"},{"location":"okd_install/#create-the-install-config","text":"Open a command or terminal window and switch to the directory containing the openshift-install binary run command ./openshift-install create install-config , and enter the values when prompted - these are the values you prepared at the top of this page. select oVirt as the Platform enter the engine F ully Q ualified D omain N ame accept the suggested engine username (admin@internal) by pressing enter enter the engine password accept the default Cluster name, which should be set as Default unless you have changed oVirt configuration select the appropriate storage domain to store the OKD cluster host disk images select the Network to use input IP address allocated to OKD cluster API endpoint input IP address allocated to OKD cluster Ingress input the base domain input the cluster name (forms the end of all URLs for cluster apps apps.<cluster name>.<basse domain> ) input the pull secret (either a valid Red Hat or the fake one provided above) Warning There is a bug in the OVNKubernetes network provider in OKD 4.7. This prevents the install working without manual intervention and prevents the detection of external IP addresses causing the worker Machines being stuck in Provisioning phase. A work around is to use the OpenShiftSDN network provider. To do this change the install-config.yaml file created in the previous steps and change the networkType property from OVNKubernetes to OpenShiftSDN networking : clusterNetwork : - cidr : 10.128.0.0/14 hostPrefix : 23 machineNetwork : - cidr : 10.0.0.0/16 networkType : OpenShiftSDN serviceNetwork : - 172.30.0.0/16 Warning From OKD 4.8 the default install configuration now defines affinity groups. If you do not have at least 3 physical oVirt hosts then you need to alter the install configuration to not enforce the affinity groups. To do this in the platform section at the bottom of the install-config.yaml file, set the enforcing property to false for the affinity groups: platform : ovirt : affinityGroups : - description : AffinityGroup for spreading each compute machine to a different host enforcing : false name : compute priority : 3 - description : AffinityGroup for spreading each control plane machine to a different host enforcing : false name : controlplane priority : 5","title":"Create the install config"},{"location":"okd_install/#run-the-installer","text":"Note The default install will download the required content from the internet. It is assumed that the machine running the openshift-install application and the machine hosting oVirt and the OKD cluster have a good internet connection to be able to download the required content. create an install directory mkdir install copy the configuration into the install directory - don't move it, copy it as it gets deleted during the install, so it is good to have a backup copy. cp install-config.yaml install create the cluster with command ./openshift-install create cluster --dir=./install --log-level=info wait until the setup completes, this can take quite a while, 30-60 minutes is not uncommon when the install completes you will see a message similar to this: INFO Install complete! INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/Users/brian/projects/okd/4.7/install/auth/kubeconfig' INFO Access the OpenShift web-console here: https://console-openshift-console.apps.okd.lab.home INFO Login to the console with user: \"kubeadmin\", and password: \"xxxxx-xxxxx-xxxxx-xxxxx\" Warning It is important that you keep the install directory after the installation has finished, as if you ever shut your cluster down for a length of time, then the certificates used within the system may expire whilst the system is down. The only way to access the cluster to renew the certificates is with the auth credentials contained within the install directory. You also need the install directory if you want to uninstall the cluster. Info If you have a slower network connection it is possible that the install will timeout. You can continue the install process if the install times out. Depending on the place where it timeouts you can continue: Timeout during bootstrap : ./openshift-install wait-for bootstrap-complete once bootstrap completes run command : ./openshift-install destroy bootstrap to clean up the bootstrap vm the run the command to wait for the remaining install ./openshift-install wait-for install-complete Timeout after bootstrap completes, waiting for cluster to initialize run command : ./openshift-install wait-for install-complete","title":"Run the installer"},{"location":"okd_install/#connecting-to-the-cluster","text":"Now your cluster has been installed you want to be able to access it. There are 2 ways to access it: Web Console Command line","title":"Connecting to the cluster"},{"location":"okd_install/#web-ui","text":"After installation your cluster web console should be accessible at the URL displayed when the openshift-install command completes. The URL is of the form https://console-openshift-console.apps.<cluster name>.<base domain> . You can use the kubeadmin command, with the password displayed when the openshift-install command completed. This is the temporary cluster admin account. You should add an identity provider and create your own admin account. Instructions to complete this task are provided below.","title":"Web UI"},{"location":"okd_install/#command-lined","text":"You can access the cluster from the command line using the oc command line tool. Please ensure the version of OC is up to date. The correct version of oc for your cluster can be downloaded from the web console. Select the question mark (?) icon next to your user name in the top menu, then select Command line tools from the menu. Then you can select the appropriate binary version of the tool for your operating system. Once you have the oc tool install you can connect to the cluster. There are 2 ways to connect to the cluster:","title":"Command lined"},{"location":"okd_install/#post-install-tasks","text":"Info If you are getting warnings that your cluster is unable to retrieve available updates because https://origin-release.svc.ci.openshift.org host cannot be found, then you have an out of date URL in the update service. You can fix this by running the following command as an admin user on the command line: oc patch clusterversion/version --patch '{\"spec\":{\"upstream\":\"https://amd64.origin.releases.ci.openshift.org/graph\"}}' --type=merge","title":"Post install tasks"},{"location":"okd_install/#adding-an-identity-provider","text":"You need a system to authenticate users to the system. OKD supports a number of different options for authenticating users. Here we will use the basic htpasswd authentication method, which is a simple list of users with encrypted passwords, held in a Kubernetes secrets object. Details of other authentication methods can be found in the OKD documentation Full instructions for the HTPasswd identity provider can be found in the OKD documentation","title":"Adding an Identity provider"},{"location":"okd_install/#creating-an-administrator-account","text":"After adding the identity provider, you want to make one of the accounts added to the identity provider an administrator user. Before proceeding you should log into the cluster from the Web Console, using the user account you want to make an admin user. On the command line you can make the user an admin user with the following command (replacing with a valid user name created for your configured identity provider): oc adm policy add-cluster-role-to-user cluster-admin <user>","title":"Creating an administrator account"},{"location":"overview/","text":"Overview \u00b6 Getting skills in Cloud Native Development and maintaining those skills can be challenging for a developer. The technology is evolving very quickly, so having an environment where a developer can update, explore and learn new features as they emerge is a valuable resource. Managed cloud based environments are usually a shared resource across a development team, where it is not appropriate to upgrade to the latest emerging technology. as the development environment needs to be a managed, stable environment. Costs can also be an inhibitor to allow developers to have the environments they want to explore and keep up to date, especially when doing so at home. This project documents my cloud native development home lab. I wanted to only use freely available open source technology, but also technology where there is a route into a production environment. Although the project doesn't use any managed cloud resources, it does require a certain amount of hardware - which may not be appropriate for all developers. I purchased a reconditioned workstation with a large amount of memory and Intel Xeon processors, to have sufficient hardware resources to run a cloud environment. This environment will not run on a typical home computer. If you don't have sufficient hardware resources to install a home lab environment, then CodeReady Containers (CRC) may be a solution, as this will run on a laptop or workstation with at least 16GB memory. There are 2 versions of CRC available, the OpenShift version and the version built on top of OKD This project creates a home lab environment based on 2 primary open source projects : oVirt - which is an upstream community run project contributing in to Red Hat Virtualization OKD - which is a community run sibling project to Red Hat OpenShift , taking the same source code as a starting point, then applying some customisations to create the deliverables. oVirt \u00b6 oVirt is an open-source virtualization solution based on the KVM hypervisor. It offers an open-source solution similar to the enterprise products from VMWare, to run a distributed, virtualization solution. In this project, the setup will focus on a home lab deployment, using only a single oVirt host. However, if required larger configurations could be used. OKD \u00b6 OKD is an open-source distribution of Kubernetes, that delivers enhanced capabilities over a standard Kubernetes platform. With additional security features and an integrated user interface with both admin and developer features.","title":"Overview"},{"location":"overview/#overview","text":"Getting skills in Cloud Native Development and maintaining those skills can be challenging for a developer. The technology is evolving very quickly, so having an environment where a developer can update, explore and learn new features as they emerge is a valuable resource. Managed cloud based environments are usually a shared resource across a development team, where it is not appropriate to upgrade to the latest emerging technology. as the development environment needs to be a managed, stable environment. Costs can also be an inhibitor to allow developers to have the environments they want to explore and keep up to date, especially when doing so at home. This project documents my cloud native development home lab. I wanted to only use freely available open source technology, but also technology where there is a route into a production environment. Although the project doesn't use any managed cloud resources, it does require a certain amount of hardware - which may not be appropriate for all developers. I purchased a reconditioned workstation with a large amount of memory and Intel Xeon processors, to have sufficient hardware resources to run a cloud environment. This environment will not run on a typical home computer. If you don't have sufficient hardware resources to install a home lab environment, then CodeReady Containers (CRC) may be a solution, as this will run on a laptop or workstation with at least 16GB memory. There are 2 versions of CRC available, the OpenShift version and the version built on top of OKD This project creates a home lab environment based on 2 primary open source projects : oVirt - which is an upstream community run project contributing in to Red Hat Virtualization OKD - which is a community run sibling project to Red Hat OpenShift , taking the same source code as a starting point, then applying some customisations to create the deliverables.","title":"Overview"},{"location":"overview/#ovirt","text":"oVirt is an open-source virtualization solution based on the KVM hypervisor. It offers an open-source solution similar to the enterprise products from VMWare, to run a distributed, virtualization solution. In this project, the setup will focus on a home lab deployment, using only a single oVirt host. However, if required larger configurations could be used.","title":"oVirt"},{"location":"overview/#okd","text":"OKD is an open-source distribution of Kubernetes, that delivers enhanced capabilities over a standard Kubernetes platform. With additional security features and an integrated user interface with both admin and developer features.","title":"OKD"},{"location":"ovirt/","text":"oVirt installation and setup \u00b6 Install oVirt node \u00b6 download ISO . You want the oVirt Node ISO. flash to USB memory stick Boot from memory stick Run oVirt setup wizard \u00b6 Select language from the list then press the Continue button Select the keyboard layout then press the Done key Enter the Installation Destination section you need to keep a disk or partition free for Gluster to use (this was discussed in the preparation section) Enter a root password Set the TimeZone Enter the Network and Host name section (ovirt host details - z840.home.lab in my setup) select Configure to configure the network interface In the IPv4 tab select Manual configuration then select Add to enter the configuration - this should match what is in your DNS configuration. Press Save to store the IP configuration Click the toggle switch to enable the Ethernet interface Select Done to complete the network configuration (optional) If you want to configure NTP time synchronisation, revisit the Time & Date section and configure the NTP server to a local server then enable NTP Select Begin Installation to start the installation Wait for installation to complete then select to reboot when selected (ensure the USB memory stick is removed so it doesn't boot from the install media) You can now interact with the installed system using the web based cockpit UI. On your laptop or a workstation on the network, navigate to https://<host address>:9090 . Where the host address is the address you added in DNS for the IP address you manually configured in step 6 above. (Optional) Create partition for Gluster filesystem \u00b6 If you are using a disk partition for the Gluster FS, then you need to create the partition. In the cockpit UI, navigate to the Storage section. Select the disk where the partition will live from the side panel. This will show the partitions in the Content section. Next to the unallocated space there is a Create partition button. Click the button to create a partition. Set the required size and select No filesystem as the type then click Create partition to create the partition. Setup ssh keys \u00b6 During the install the ansible scripts need to be able to execute commands on the host. To do this it uses passwordless ssh. Even though we are only using a single host, the scripts are written to work across multiple hosts, so we need to enable passwordless ssh. You need to know your host IP address to substitute into ssh-copy-id command. You can find your host address using command hostname -I Enter the following commands using the terminal section of the cockpit web console ( http://<host-address>:9090 ) for the oVirt host. Substitute your host address in the copy command, accept all defaults and leave the passphrase blank: ssh-keygen ssh-copy-id root@<host-address> you will be prompted to confirm the connection, answer yes then you will be prompted for the root password, which you entered in step 4 of the previous section. Configure LVM filter \u00b6 By default Logical Volumes are configured to specific devices, so you need to add the device you want to use for the Gluster storage. Using the terminal section in the cockpit web ui, edit file /etc/lvm/lvm.conf and search for a line the starts filter = , not lines starting with # are comments. Modify the line to include your chosen device for the gluster volumes, eg. If the filter is currently set to filter = [\"a|^/dev/disk/by-id/lvm-pv-uuid-U77HZ9-LPry-OOMY-bgOq-t34w-l3av-srg8tU$|\", \"r|.*|\"] and your chosen device is /dev/sdb , then the filter line needs to be modified to: filter = [\"a|^/dev/disk/by-id/lvm-pv-uuid-U77HZ9-LPry-OOMY-bgOq-t34w-l3av-srg8tU$|\", \"a|^/dev/sdb|\", \"r|.*|\"] Save the modified lvm.conf file Note If you plan to use an entire disk for GlusterFS, then it is important that the disk is not partitioned, so if it has previously been used and has a partition table on it, then use the Terminal section of the cockpit interface to clear the device. E.g. if you will be using the disk /dev/sdb for gluster, then wipe the disk using command wipefs -a -f /dev/sdb . This will erase the disk. Setup the hyperconverged oVirt Hosted Engine and Gluster storage \u00b6 The hyperconverged Hosted Engine and Gluster storage can be installed using the cockpit web console ( http://<host-address>:9090 ). In the Cockpit UI navigate to the Virtualization section in the side menu, then select the Hosted Engine section. Select the start button under Hyperconverged. Select the Run Gluster Wizard for Single Node option to start the install. Note Step 5 is only required in oVirt versions 4.4.6 and earlier, from 4.4.7 the installer works without needing to modify the generated config Enter the Fully qualified hostname for the oVirt cluster (this should be the name of the oVirt node that was used during the ssh-copy-id command) You can leave the Packages options as default You can leave the Volumes options as default. However, if you have a small amount of storage you may want to delete 2 of the 3 default volumes, so all available storage will be in a single volume. In the Bricks options, set the device to the device you have available for the gluster storage for all the configured volumes. This could be an entire disk (e.g. /dev/sdb), or an available disk partition (/dev/sda3) (only for earlier versions of oVirt) In the review section edit the summary and remove the line - 5900/tcp , as this will cause the scripts to fail. Save the script Select the Deploy button to start the Gluster Storage installation If all works you should see the screen confirming the Gluster installation was successful You can now select the Continue to Hosted Engine Deployment button to install the Hosted Engine. Install Hosted Engine \u00b6 The Hosted Engine will try to add port 6900 to the public zone and will fail as the port is already being exposed. Before installing the Hosted Engine you need to modify the firewall configuration. Modify the firewall rules \u00b6 Info This step was needed in versions of ovirt before 4.4.9, but is no longer needed with 4.4.10 You need to modify the firewall config to remove port 6900 to ensure the automated deployment will work. Switch to the terminal section and edit the file /etc/firewalld/zones/public.xml to have the following content: <zone> <short> Public </short> <description> For use in public areas. You do not trust the other computers on networks to not harm your computer. Only selected incoming connections are accepted. </description> <service name= \"ssh\" /> <service name= \"dhcpv6-client\" /> <service name= \"cockpit\" /> <service name= \"libvirt\" /> <service name= \"libvirt-tls\" /> <service name= \"glusterfs\" /> <service name= \"custom-vdsm\" /> <port port= \"2049\" protocol= \"tcp\" /> <port port= \"54321\" protocol= \"tcp\" /> <port port= \"5900-6899\" protocol= \"tcp\" /> <port port= \"6901-6923\" protocol= \"tcp\" /> <port port= \"5666\" protocol= \"tcp\" /> <port port= \"16514\" protocol= \"tcp\" /> </zone> You can see the port range 5900-6923 has been split into 2 ranges avoiding port 6900 run command systemctl restart firewalld to make the new firewall configuration live. You can now return to the Virtualization section of the Cockpit web console to continue with the Hosted Engine deployment. Install the Hosted Engine \u00b6 Select to Deploy the Hosted Engine In the VM Settings section enter: the fully qualified host name for the hosted engine ( virt.home.lab in my setup) switch to static Network Configuration and set the IP address to the value matching your hostname in your DNS, the gateway address and DNS server address enter the root password you want to set for the hosted engine operating system select Next to move to Engine section In the Engine section enter the admin password you want to set for the Engine, then select **Next to move to the next section Review the configuration information then press Prepare VM to deploy the Hosted Engine virtual machine in the Prepare VM section, this can take a while When the VM has been setup press Next to complete the setup In the storage section the settings should be pre-populated, so you can press Next then Finish Deployment Setup Console viewing app \u00b6 oVirt uses the SPICE protocol to access a hosted virtual machine. You need to have a console viewing application installed on your system. Details can be found here oVirt machine console on macOS \u00b6 Setting up the console viewer on MacOS can be a little more involved. When using Homebrew , issue the following commands on the command line: brew install gtk+3 brew tap jeffreywildman/homebrew-virt-manager brew install remoteviewer then follow the instructions to setup the association of .vv files to the remote-viewer application.","title":"oVirt setup"},{"location":"ovirt/#ovirt-installation-and-setup","text":"","title":"oVirt installation and setup"},{"location":"ovirt/#install-ovirt-node","text":"download ISO . You want the oVirt Node ISO. flash to USB memory stick Boot from memory stick","title":"Install oVirt node"},{"location":"ovirt/#run-ovirt-setup-wizard","text":"Select language from the list then press the Continue button Select the keyboard layout then press the Done key Enter the Installation Destination section you need to keep a disk or partition free for Gluster to use (this was discussed in the preparation section) Enter a root password Set the TimeZone Enter the Network and Host name section (ovirt host details - z840.home.lab in my setup) select Configure to configure the network interface In the IPv4 tab select Manual configuration then select Add to enter the configuration - this should match what is in your DNS configuration. Press Save to store the IP configuration Click the toggle switch to enable the Ethernet interface Select Done to complete the network configuration (optional) If you want to configure NTP time synchronisation, revisit the Time & Date section and configure the NTP server to a local server then enable NTP Select Begin Installation to start the installation Wait for installation to complete then select to reboot when selected (ensure the USB memory stick is removed so it doesn't boot from the install media) You can now interact with the installed system using the web based cockpit UI. On your laptop or a workstation on the network, navigate to https://<host address>:9090 . Where the host address is the address you added in DNS for the IP address you manually configured in step 6 above.","title":"Run oVirt setup wizard"},{"location":"ovirt/#optional-create-partition-for-gluster-filesystem","text":"If you are using a disk partition for the Gluster FS, then you need to create the partition. In the cockpit UI, navigate to the Storage section. Select the disk where the partition will live from the side panel. This will show the partitions in the Content section. Next to the unallocated space there is a Create partition button. Click the button to create a partition. Set the required size and select No filesystem as the type then click Create partition to create the partition.","title":"(Optional) Create partition for Gluster filesystem"},{"location":"ovirt/#setup-ssh-keys","text":"During the install the ansible scripts need to be able to execute commands on the host. To do this it uses passwordless ssh. Even though we are only using a single host, the scripts are written to work across multiple hosts, so we need to enable passwordless ssh. You need to know your host IP address to substitute into ssh-copy-id command. You can find your host address using command hostname -I Enter the following commands using the terminal section of the cockpit web console ( http://<host-address>:9090 ) for the oVirt host. Substitute your host address in the copy command, accept all defaults and leave the passphrase blank: ssh-keygen ssh-copy-id root@<host-address> you will be prompted to confirm the connection, answer yes then you will be prompted for the root password, which you entered in step 4 of the previous section.","title":"Setup ssh keys"},{"location":"ovirt/#configure-lvm-filter","text":"By default Logical Volumes are configured to specific devices, so you need to add the device you want to use for the Gluster storage. Using the terminal section in the cockpit web ui, edit file /etc/lvm/lvm.conf and search for a line the starts filter = , not lines starting with # are comments. Modify the line to include your chosen device for the gluster volumes, eg. If the filter is currently set to filter = [\"a|^/dev/disk/by-id/lvm-pv-uuid-U77HZ9-LPry-OOMY-bgOq-t34w-l3av-srg8tU$|\", \"r|.*|\"] and your chosen device is /dev/sdb , then the filter line needs to be modified to: filter = [\"a|^/dev/disk/by-id/lvm-pv-uuid-U77HZ9-LPry-OOMY-bgOq-t34w-l3av-srg8tU$|\", \"a|^/dev/sdb|\", \"r|.*|\"] Save the modified lvm.conf file Note If you plan to use an entire disk for GlusterFS, then it is important that the disk is not partitioned, so if it has previously been used and has a partition table on it, then use the Terminal section of the cockpit interface to clear the device. E.g. if you will be using the disk /dev/sdb for gluster, then wipe the disk using command wipefs -a -f /dev/sdb . This will erase the disk.","title":"Configure LVM filter"},{"location":"ovirt/#setup-the-hyperconverged-ovirt-hosted-engine-and-gluster-storage","text":"The hyperconverged Hosted Engine and Gluster storage can be installed using the cockpit web console ( http://<host-address>:9090 ). In the Cockpit UI navigate to the Virtualization section in the side menu, then select the Hosted Engine section. Select the start button under Hyperconverged. Select the Run Gluster Wizard for Single Node option to start the install. Note Step 5 is only required in oVirt versions 4.4.6 and earlier, from 4.4.7 the installer works without needing to modify the generated config Enter the Fully qualified hostname for the oVirt cluster (this should be the name of the oVirt node that was used during the ssh-copy-id command) You can leave the Packages options as default You can leave the Volumes options as default. However, if you have a small amount of storage you may want to delete 2 of the 3 default volumes, so all available storage will be in a single volume. In the Bricks options, set the device to the device you have available for the gluster storage for all the configured volumes. This could be an entire disk (e.g. /dev/sdb), or an available disk partition (/dev/sda3) (only for earlier versions of oVirt) In the review section edit the summary and remove the line - 5900/tcp , as this will cause the scripts to fail. Save the script Select the Deploy button to start the Gluster Storage installation If all works you should see the screen confirming the Gluster installation was successful You can now select the Continue to Hosted Engine Deployment button to install the Hosted Engine.","title":"Setup the hyperconverged oVirt Hosted Engine and Gluster storage"},{"location":"ovirt/#install-hosted-engine","text":"The Hosted Engine will try to add port 6900 to the public zone and will fail as the port is already being exposed. Before installing the Hosted Engine you need to modify the firewall configuration.","title":"Install Hosted Engine"},{"location":"ovirt/#modify-the-firewall-rules","text":"Info This step was needed in versions of ovirt before 4.4.9, but is no longer needed with 4.4.10 You need to modify the firewall config to remove port 6900 to ensure the automated deployment will work. Switch to the terminal section and edit the file /etc/firewalld/zones/public.xml to have the following content: <zone> <short> Public </short> <description> For use in public areas. You do not trust the other computers on networks to not harm your computer. Only selected incoming connections are accepted. </description> <service name= \"ssh\" /> <service name= \"dhcpv6-client\" /> <service name= \"cockpit\" /> <service name= \"libvirt\" /> <service name= \"libvirt-tls\" /> <service name= \"glusterfs\" /> <service name= \"custom-vdsm\" /> <port port= \"2049\" protocol= \"tcp\" /> <port port= \"54321\" protocol= \"tcp\" /> <port port= \"5900-6899\" protocol= \"tcp\" /> <port port= \"6901-6923\" protocol= \"tcp\" /> <port port= \"5666\" protocol= \"tcp\" /> <port port= \"16514\" protocol= \"tcp\" /> </zone> You can see the port range 5900-6923 has been split into 2 ranges avoiding port 6900 run command systemctl restart firewalld to make the new firewall configuration live. You can now return to the Virtualization section of the Cockpit web console to continue with the Hosted Engine deployment.","title":"Modify the firewall rules"},{"location":"ovirt/#install-the-hosted-engine","text":"Select to Deploy the Hosted Engine In the VM Settings section enter: the fully qualified host name for the hosted engine ( virt.home.lab in my setup) switch to static Network Configuration and set the IP address to the value matching your hostname in your DNS, the gateway address and DNS server address enter the root password you want to set for the hosted engine operating system select Next to move to Engine section In the Engine section enter the admin password you want to set for the Engine, then select **Next to move to the next section Review the configuration information then press Prepare VM to deploy the Hosted Engine virtual machine in the Prepare VM section, this can take a while When the VM has been setup press Next to complete the setup In the storage section the settings should be pre-populated, so you can press Next then Finish Deployment","title":"Install the Hosted Engine"},{"location":"ovirt/#setup-console-viewing-app","text":"oVirt uses the SPICE protocol to access a hosted virtual machine. You need to have a console viewing application installed on your system. Details can be found here","title":"Setup Console viewing app"},{"location":"ovirt/#ovirt-machine-console-on-macos","text":"Setting up the console viewer on MacOS can be a little more involved. When using Homebrew , issue the following commands on the command line: brew install gtk+3 brew tap jeffreywildman/homebrew-virt-manager brew install remoteviewer then follow the instructions to setup the association of .vv files to the remote-viewer application.","title":"oVirt machine console on macOS"},{"location":"preparation/","text":"Getting ready for the installation \u00b6 Before starting to install oVirt and OKD you need to ensure you have the necessary environment available. This includes: Having sufficient compute, memory and storage available for the cluster Having the required networking infrastructure and services available and configured Hardware requirements \u00b6 The OKD documentation describes an OKD production environment on oVirt, which specifies hardware requirements of: Minimum 28 vCPUs 112 GiB RAM min 230 GiB storage (non-prod) or 840 GiB (prod) The documentation also assumes that oVirt will be installed over multiple physical hosts, to provide a more resilient environment. However, for this home lab setup a single machine will be used. However, for a Home Lab environment the oVirt environment can be configured to over commit memory, to allow a cluster to be setup with a minimum of 56 GiB memory, but then you will be constrained on what you can deploy into the cluster. Todo When SNO docs is available fix There is also the option to not deploy a fault tolerant OKD cluster and reduce the number of Master nodes, details of how to create a single node installation can be found in the [OKD documentation](https://docs.okd.io/latest/installing/installing_sno/install-sno-preparing-to-install-sno.html){target=_blank} , which needs 8 vCPU cores and 32GB RAM. An alternate option may be to use Code-Ready Containers Storage setup \u00b6 A fault tolerant version of OKD needs fast storage to keep the nodes synchronised via the etcd key-value store. Traditional spinning disks are not fast enough for this purpose so Solid State Drives (SSD) are needed to host the system nodes. oVirt includes glusterfs to provide host storage to the virtual machines. The SSD storage needs to be exposed via gluster so oVirt can use it as the storage for the OKD cluster nodes Network Requirements \u00b6 To run oVirt and OKD you need some local network infrastructure to provide: DHCP IP allocation DNS name resolution each physical host you will use to host the oVirt nodes will need an IP address the oVirt virtualization manager VM will need an IP address, with DNS resolution on the local network OKD needs 2 IP addresses allocated for the API endpoint and the cluster ingress, again DNS resolution is needed for the 2 IP addresses the local network DNS resolution must be configured to resolve certain FQDN (Fully Qualified Domain Name). This includes resolving all names matching *.apps.<cluster-name>.<base-domain> , which will resolve all deployed application on OKD exposed using the ingress.","title":"Preparing"},{"location":"preparation/#getting-ready-for-the-installation","text":"Before starting to install oVirt and OKD you need to ensure you have the necessary environment available. This includes: Having sufficient compute, memory and storage available for the cluster Having the required networking infrastructure and services available and configured","title":"Getting ready for the installation"},{"location":"preparation/#hardware-requirements","text":"The OKD documentation describes an OKD production environment on oVirt, which specifies hardware requirements of: Minimum 28 vCPUs 112 GiB RAM min 230 GiB storage (non-prod) or 840 GiB (prod) The documentation also assumes that oVirt will be installed over multiple physical hosts, to provide a more resilient environment. However, for this home lab setup a single machine will be used. However, for a Home Lab environment the oVirt environment can be configured to over commit memory, to allow a cluster to be setup with a minimum of 56 GiB memory, but then you will be constrained on what you can deploy into the cluster. Todo When SNO docs is available fix There is also the option to not deploy a fault tolerant OKD cluster and reduce the number of Master nodes, details of how to create a single node installation can be found in the [OKD documentation](https://docs.okd.io/latest/installing/installing_sno/install-sno-preparing-to-install-sno.html){target=_blank} , which needs 8 vCPU cores and 32GB RAM. An alternate option may be to use Code-Ready Containers","title":"Hardware requirements"},{"location":"preparation/#storage-setup","text":"A fault tolerant version of OKD needs fast storage to keep the nodes synchronised via the etcd key-value store. Traditional spinning disks are not fast enough for this purpose so Solid State Drives (SSD) are needed to host the system nodes. oVirt includes glusterfs to provide host storage to the virtual machines. The SSD storage needs to be exposed via gluster so oVirt can use it as the storage for the OKD cluster nodes","title":"Storage setup"},{"location":"preparation/#network-requirements","text":"To run oVirt and OKD you need some local network infrastructure to provide: DHCP IP allocation DNS name resolution each physical host you will use to host the oVirt nodes will need an IP address the oVirt virtualization manager VM will need an IP address, with DNS resolution on the local network OKD needs 2 IP addresses allocated for the API endpoint and the cluster ingress, again DNS resolution is needed for the 2 IP addresses the local network DNS resolution must be configured to resolve certain FQDN (Fully Qualified Domain Name). This includes resolving all names matching *.apps.<cluster-name>.<base-domain> , which will resolve all deployed application on OKD exposed using the ingress.","title":"Network Requirements"},{"location":"resources/","text":"Additional Resources \u00b6 Below are some links and posts about OKD. They may provide additional insight and options to help your OKD learning. Community locations \u00b6 OKD main site OKD git repo OKD git community repo Installation and setup help \u00b6 Craig Robinson - Installing an OKD 4.5 Cluster Ross Brigoli - Running OpenShift at Home Josphat Mutal - How to Setup Local OpenShift Origin (OKD) Cluster on CentOS 7 Oren Oichman - Deploy OKD in a disconnected (Air Gap) Environment Josphat Mutal - How to install OKD OpenShift 4.7 Cluster on OpenStack OKD 4 additional content \u00b6 OKD 4 Testing and Deployment Workshop - Videos and Additional Resources OKD 4 blogs on OpenShift Blog site","title":"Resources"},{"location":"resources/#additional-resources","text":"Below are some links and posts about OKD. They may provide additional insight and options to help your OKD learning.","title":"Additional Resources"},{"location":"resources/#community-locations","text":"OKD main site OKD git repo OKD git community repo","title":"Community locations"},{"location":"resources/#installation-and-setup-help","text":"Craig Robinson - Installing an OKD 4.5 Cluster Ross Brigoli - Running OpenShift at Home Josphat Mutal - How to Setup Local OpenShift Origin (OKD) Cluster on CentOS 7 Oren Oichman - Deploy OKD in a disconnected (Air Gap) Environment Josphat Mutal - How to install OKD OpenShift 4.7 Cluster on OpenStack","title":"Installation and setup help"},{"location":"resources/#okd-4-additional-content","text":"OKD 4 Testing and Deployment Workshop - Videos and Additional Resources OKD 4 blogs on OpenShift Blog site","title":"OKD 4 additional content"},{"location":"workstation/","text":"Hosted workstation \u00b6 It can be convenient to have a workstation image running on oVirt that you can remote in to. This workstation will run the latest fedora workstation , which at the time of writing this is fedora 35 (Fedora-Workstation-Live-x86_64-35-1.2.iso) Download iso upload to oVirt storage From the ovirt Administration Portal, goto the Storage -> Disks section, then select Upload then Start from the top menu Choose the fedora iso file to upload, then select the storage domain to store the file in and press OK to start the upload wait for the upload to complete","title":"Hosted workstation"},{"location":"workstation/#hosted-workstation","text":"It can be convenient to have a workstation image running on oVirt that you can remote in to. This workstation will run the latest fedora workstation , which at the time of writing this is fedora 35 (Fedora-Workstation-Live-x86_64-35-1.2.iso) Download iso upload to oVirt storage From the ovirt Administration Portal, goto the Storage -> Disks section, then select Upload then Start from the top menu Choose the fedora iso file to upload, then select the storage domain to store the file in and press OK to start the upload wait for the upload to complete","title":"Hosted workstation"},{"location":"devops/","text":"DevOps environment \u00b6 After setting up OKD cluster additional components will be installed to create a Cloud Native Development environment. The components that will be installed are: Noobaa (Object storage) Project Quay (Container Repository) GitOps (ArgoCD) Tekton (OpenShift Pipelines) Gitea (Source control) Web Terminal Che (CodeReady Workspaces)","title":"DevOps"},{"location":"devops/#devops-environment","text":"After setting up OKD cluster additional components will be installed to create a Cloud Native Development environment. The components that will be installed are: Noobaa (Object storage) Project Quay (Container Repository) GitOps (ArgoCD) Tekton (OpenShift Pipelines) Gitea (Source control) Web Terminal Che (CodeReady Workspaces)","title":"DevOps environment"},{"location":"devops/argocd/","text":"ArgoCD \u00b6 Argo CD is a GitOps continuous delivery tool for Kubernetes. It allows you to declaratively define kubernetes and application configuration in a source code management tool and will ensure ensure the cluster remains in sync with the declared desired state. Installation \u00b6 ArgoCD is in the community operator catalog, so can be easily installed. Install the operator, accepting all the defaults. When the operator is installed, create an ArgoCD instance accepting all defaults. Accessing ArgoCD \u00b6 In the ArgoCD overview panel of the deployed instance (Installed Operators -> ArgoCD (ensure All Projects is the active project) -> ArgoCD -> select your instance) you will see the project that your ArgoCD instance is installed in (default should be openshift-operators). Switch to the project that ArgoCD is installed in open the secrets panel (Workloads -> Secrets) and find the secret <ArgoCD instance name>-cluster . This secret contains the admin password for your ArgoCD instance. The URL for the cluster can be found in the routes (Networking -> Routes)","title":"ArgoCD"},{"location":"devops/argocd/#argocd","text":"Argo CD is a GitOps continuous delivery tool for Kubernetes. It allows you to declaratively define kubernetes and application configuration in a source code management tool and will ensure ensure the cluster remains in sync with the declared desired state.","title":"ArgoCD"},{"location":"devops/argocd/#installation","text":"ArgoCD is in the community operator catalog, so can be easily installed. Install the operator, accepting all the defaults. When the operator is installed, create an ArgoCD instance accepting all defaults.","title":"Installation"},{"location":"devops/argocd/#accessing-argocd","text":"In the ArgoCD overview panel of the deployed instance (Installed Operators -> ArgoCD (ensure All Projects is the active project) -> ArgoCD -> select your instance) you will see the project that your ArgoCD instance is installed in (default should be openshift-operators). Switch to the project that ArgoCD is installed in open the secrets panel (Workloads -> Secrets) and find the secret <ArgoCD instance name>-cluster . This secret contains the admin password for your ArgoCD instance. The URL for the cluster can be found in the routes (Networking -> Routes)","title":"Accessing ArgoCD"},{"location":"devops/gitea/","text":"Gitea \u00b6 Gitea is an opensource git server that will be used to provide a local git server. There is a RedHat provided operator available as an Operator Hub catalog source on GitHub , however, this will not work on OKD as it relies on a container registry.redhat.io/openshift4/ose-kube-rbac-proxy, which requires an OpenShift pull secret to access. An kube RBAC proxy is available here : https://github.com/brancz/kube-rbac-proxy - is this an equivalent replacement? Container is available also here","title":"Gitea"},{"location":"devops/gitea/#gitea","text":"Gitea is an opensource git server that will be used to provide a local git server. There is a RedHat provided operator available as an Operator Hub catalog source on GitHub , however, this will not work on OKD as it relies on a container registry.redhat.io/openshift4/ose-kube-rbac-proxy, which requires an OpenShift pull secret to access. An kube RBAC proxy is available here : https://github.com/brancz/kube-rbac-proxy - is this an equivalent replacement? Container is available also here","title":"Gitea"},{"location":"devops/objectStorage/","text":"Object Storage \u00b6 Object storage is a popular storage technology on cloud, with all major cloud providers offering an Object Storage service. The Amazon Web Services S3 API has become the de facto API to use to access Object Service. noobaa is an Open Source project that offers AWS S3 compatibility and offers a private cloud Object Storage capability with the ability to aggregate private and public object storage services. Installing noobaa \u00b6 Installing noobaa on OKD is simple, using the NooBaa Operator . Simply follow the instructions in the project README Todo Are there Windows instructions or should Windows users use the Windows Linux integration? Managing noobaa \u00b6 Once noobaa is installed you can access the management console and the S3 API. To get the URLs you can use the OKD command line oc get routes -n noobaa or the noobaa command line noobaa status or use the web console to look at the routes in the noobaa project Note The noobaa management console only runs in the Chrome browser To log into the management console you can use the details from the noobaa-admin secret in the noobaa project. Once logged onto the management console you can create your own user account and create buckets, backing stores or add other public cloud providers","title":"Object Storage"},{"location":"devops/objectStorage/#object-storage","text":"Object storage is a popular storage technology on cloud, with all major cloud providers offering an Object Storage service. The Amazon Web Services S3 API has become the de facto API to use to access Object Service. noobaa is an Open Source project that offers AWS S3 compatibility and offers a private cloud Object Storage capability with the ability to aggregate private and public object storage services.","title":"Object Storage"},{"location":"devops/objectStorage/#installing-noobaa","text":"Installing noobaa on OKD is simple, using the NooBaa Operator . Simply follow the instructions in the project README Todo Are there Windows instructions or should Windows users use the Windows Linux integration?","title":"Installing noobaa"},{"location":"devops/objectStorage/#managing-noobaa","text":"Once noobaa is installed you can access the management console and the S3 API. To get the URLs you can use the OKD command line oc get routes -n noobaa or the noobaa command line noobaa status or use the web console to look at the routes in the noobaa project Note The noobaa management console only runs in the Chrome browser To log into the management console you can use the details from the noobaa-admin secret in the noobaa project. Once logged onto the management console you can create your own user account and create buckets, backing stores or add other public cloud providers","title":"Managing noobaa"},{"location":"devops/pipelines/","text":"Pipelines \u00b6 Tekton is the upstream project for OpenShift Pipelines. The upstream project includes the integration with the OpenShift and OKD consoles, however the upstream project doesn't include the cluster wide tasks that are included in the OpenShift Pipelines operator. Installing Tekton \u00b6 The OpenShift operator is not available in the community hub, but the Tekton site includes install instructions for Tekton Tekton Hub \u00b6 There is a community hub of tasks available, which is the source of the content installed by the OpenShift Pipelines operator. The hub allows you to search for tasks then provides the command to install the task to your cluster.","title":"Pipelines"},{"location":"devops/pipelines/#pipelines","text":"Tekton is the upstream project for OpenShift Pipelines. The upstream project includes the integration with the OpenShift and OKD consoles, however the upstream project doesn't include the cluster wide tasks that are included in the OpenShift Pipelines operator.","title":"Pipelines"},{"location":"devops/pipelines/#installing-tekton","text":"The OpenShift operator is not available in the community hub, but the Tekton site includes install instructions for Tekton","title":"Installing Tekton"},{"location":"devops/pipelines/#tekton-hub","text":"There is a community hub of tasks available, which is the source of the content installed by the OpenShift Pipelines operator. The hub allows you to search for tasks then provides the command to install the task to your cluster.","title":"Tekton Hub"},{"location":"devops/registry/","text":"Container Registry \u00b6 There are many public containers registries available with dockerhub being the most popular. However, dockerhub as download limits which can cause issues for home lab environments. Without a paid subscription dockerhub can limit the number of daily container pulls from a single IP address, which can prevent a Kubernetes platform from accessing images. To overcome the dockerhub limits a local container registry will be setup that also provides a pull through cache, so containers on public registries can be fetched from the public and cached and served from the local registry to overcome the daily limit. It also provides local registry functionality rather than having to push images to a public registry for development projects. Project Quay is a community distribution of RedHat Quay. It provides a fully functional container registry with build in scanning using Clair and a pull through cache (from version 3.7) Installing project Quay \u00b6 The OperatorHub installed in OKD contains the community Project Quay operator, which will install Project Quay in your OKD cluster. Install the Project Quay operator and wait for it to be ready (accept all the default settings), then from the Installed Operators panel in the OKD web UI select the Quay operator then switch to the Quay Registry tab. Select the * Create Quay Registry button then : give the registry a name Expand the Advanced configuration section, then the Components section remove the tick from the managed checkbox for the objectstorage service press the create button to start the Quay registry creation The Quay registry will not complete at this stage as there is some configuration missing. The operator uses a secret to control the configuration of the registry. Open the Quay Registry entry (in the Installed Operators -> Quay operator -> Quay Registry section of the OKD UI). You will see a link to the configuration secret displayed. Select it to open the secret then press the Reveal values link to show the configuration document. You need to change the secret to: include the object storage config (covered in the next section) turn on the caching feature (which is disabled by default in Project Quay 3.7) by adding FEATURE_PROXY_CACHE: true to the config secret give admin access to a user (covered in later section) Configuring Quay storage \u00b6 Quay uses ObjectStorage so will use the noobaa installation installation. In the noobaa management console login and select the Buckets icon from the side menu, then create a new bucket for use by quay. Whilst in the noobaa management console, select the Overview icon from the side menu then the Connect Application button. This will display a window containing the connection details needed to communicate with noobaa. If you have created multiple accounts you can use the Target Account field to select the account to connect as. You will need the connection settings in the next step. Add the Object Store configuration to the Quay configuration secret: DISTRIBUTED_STORAGE_CONFIG : local_us : - RHOCSStorage - access_key : <access key> bucket_name : <bucket name> hostname : <hostname> is_secure : false port : \"80\" secret_key : <secret key> storage_path : /datastorage/registry where: <access key> is the access key from the noobaa connection details <bucket name> is the storage bucket created for quay to use <hostname> is the S3 API endpoint for noobaa. <secret key> is the secret key from the noobaa connection details Configuration secret \u00b6 The Project Quay documentation has more details about the configuration secret, but your secret should look something like this: ALLOW_PULLS_WITHOUT_STRICT_LOGGING : false AUTHENTICATION_TYPE : Database DEFAULT_TAG_EXPIRATION : 2w DISTRIBUTED_STORAGE_CONFIG : local_us : - RHOCSStorage - access_key : xxxx bucket_name : lab-quay hostname : s3-noobaa.apps.xxxxxx is_secure : false port : \"80\" secret_key : xxxx storage_path : /datastorage/registry ENTERPRISE_LOGO_URL : /static/img/quay-horizontal-color.svg FEATURE_BUILD_SUPPORT : true FEATURE_PROXY_CACHE : true FEATURE_DIRECT_LOGIN : true FEATURE_MAILING : false REGISTRY_TITLE : Quay REGISTRY_TITLE_SHORT : Quay SETUP_COMPLETE : true SUPER_USERS : - brian TAG_EXPIRATION_OPTIONS : - 2w TEAM_RESYNC_STALE_TIME : 60m TESTING : false You will see there is a section for SUPER_USERS. Entries in this section define which users have access to the Admin Panel. You need to create the user first. Once you have edited the secret (either using the OKD Web UI or from the command line) your ProjectQuay instance should complete, which will allow you to access the Project Quay web UI. Get the URL by looking at the Routes in the networking section of the OKD Web UI in project openshift-operators or by using the OKD command line oc get routes -n openshift-operators . There are 2 or 3 endpoints associated with quay: the main UI ( <registry name>-quay-openshift-operators.apps.<cluster base domain> ) the builder (if this feature is enabled - <registry name>-quay-builder-openshift-operators.apps.<cluster base domain> ) the config editor ( <registry name>-quay-config-editor-openshift-operators.apps.<cluster base domain> ) Open the main UI URL. This will present the login screen for Quay, which also allows you to create an account. If you want the account to have admin privileges add the username to the SUPER_USERS section of the config secret.","title":"Container Registry"},{"location":"devops/registry/#container-registry","text":"There are many public containers registries available with dockerhub being the most popular. However, dockerhub as download limits which can cause issues for home lab environments. Without a paid subscription dockerhub can limit the number of daily container pulls from a single IP address, which can prevent a Kubernetes platform from accessing images. To overcome the dockerhub limits a local container registry will be setup that also provides a pull through cache, so containers on public registries can be fetched from the public and cached and served from the local registry to overcome the daily limit. It also provides local registry functionality rather than having to push images to a public registry for development projects. Project Quay is a community distribution of RedHat Quay. It provides a fully functional container registry with build in scanning using Clair and a pull through cache (from version 3.7)","title":"Container Registry"},{"location":"devops/registry/#installing-project-quay","text":"The OperatorHub installed in OKD contains the community Project Quay operator, which will install Project Quay in your OKD cluster. Install the Project Quay operator and wait for it to be ready (accept all the default settings), then from the Installed Operators panel in the OKD web UI select the Quay operator then switch to the Quay Registry tab. Select the * Create Quay Registry button then : give the registry a name Expand the Advanced configuration section, then the Components section remove the tick from the managed checkbox for the objectstorage service press the create button to start the Quay registry creation The Quay registry will not complete at this stage as there is some configuration missing. The operator uses a secret to control the configuration of the registry. Open the Quay Registry entry (in the Installed Operators -> Quay operator -> Quay Registry section of the OKD UI). You will see a link to the configuration secret displayed. Select it to open the secret then press the Reveal values link to show the configuration document. You need to change the secret to: include the object storage config (covered in the next section) turn on the caching feature (which is disabled by default in Project Quay 3.7) by adding FEATURE_PROXY_CACHE: true to the config secret give admin access to a user (covered in later section)","title":"Installing project Quay"},{"location":"devops/registry/#configuring-quay-storage","text":"Quay uses ObjectStorage so will use the noobaa installation installation. In the noobaa management console login and select the Buckets icon from the side menu, then create a new bucket for use by quay. Whilst in the noobaa management console, select the Overview icon from the side menu then the Connect Application button. This will display a window containing the connection details needed to communicate with noobaa. If you have created multiple accounts you can use the Target Account field to select the account to connect as. You will need the connection settings in the next step. Add the Object Store configuration to the Quay configuration secret: DISTRIBUTED_STORAGE_CONFIG : local_us : - RHOCSStorage - access_key : <access key> bucket_name : <bucket name> hostname : <hostname> is_secure : false port : \"80\" secret_key : <secret key> storage_path : /datastorage/registry where: <access key> is the access key from the noobaa connection details <bucket name> is the storage bucket created for quay to use <hostname> is the S3 API endpoint for noobaa. <secret key> is the secret key from the noobaa connection details","title":"Configuring Quay storage"},{"location":"devops/registry/#configuration-secret","text":"The Project Quay documentation has more details about the configuration secret, but your secret should look something like this: ALLOW_PULLS_WITHOUT_STRICT_LOGGING : false AUTHENTICATION_TYPE : Database DEFAULT_TAG_EXPIRATION : 2w DISTRIBUTED_STORAGE_CONFIG : local_us : - RHOCSStorage - access_key : xxxx bucket_name : lab-quay hostname : s3-noobaa.apps.xxxxxx is_secure : false port : \"80\" secret_key : xxxx storage_path : /datastorage/registry ENTERPRISE_LOGO_URL : /static/img/quay-horizontal-color.svg FEATURE_BUILD_SUPPORT : true FEATURE_PROXY_CACHE : true FEATURE_DIRECT_LOGIN : true FEATURE_MAILING : false REGISTRY_TITLE : Quay REGISTRY_TITLE_SHORT : Quay SETUP_COMPLETE : true SUPER_USERS : - brian TAG_EXPIRATION_OPTIONS : - 2w TEAM_RESYNC_STALE_TIME : 60m TESTING : false You will see there is a section for SUPER_USERS. Entries in this section define which users have access to the Admin Panel. You need to create the user first. Once you have edited the secret (either using the OKD Web UI or from the command line) your ProjectQuay instance should complete, which will allow you to access the Project Quay web UI. Get the URL by looking at the Routes in the networking section of the OKD Web UI in project openshift-operators or by using the OKD command line oc get routes -n openshift-operators . There are 2 or 3 endpoints associated with quay: the main UI ( <registry name>-quay-openshift-operators.apps.<cluster base domain> ) the builder (if this feature is enabled - <registry name>-quay-builder-openshift-operators.apps.<cluster base domain> ) the config editor ( <registry name>-quay-config-editor-openshift-operators.apps.<cluster base domain> ) Open the main UI URL. This will present the login screen for Quay, which also allows you to create an account. If you want the account to have admin privileges add the username to the SUPER_USERS section of the config secret.","title":"Configuration secret"}]}