{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This site will walk you through how to create your own home or office Lab environment, enabling you to explore Red Hat OpenShift, using oVirt and OKD. This setup uses the simplest possible network configuration with a single physical workstation hosting the entire cluster. There are other online resources which explain how to set up a multi-machine hosted installation with more complex network setup. I've included a few links in the resources section of this site.","title":"Home"},{"location":"okd_install/","text":"OKD Install \u00b6 When you have your oVirt environment up and running you can install OKD. Preparing required information \u00b6 To help with the installation here is a list of information you will need during the install: item description Engine FQDN This is the Fully Qualified Domain Name of the virtual engine. This is the value you entered in step 1 in oVirt setup section Install the Hosted Engine Engine user This should be admin Engine password This is the value entered at ovirt install time, and the password you use to access the ovirt admin console oVirt Cluster name Should be Default . This can be verified in the oVirt Administration console, select Compute from the side menu then Clusters . By default there is a single cluster defined with Default as the name Storage Domain Select the storage domain with fast storage, ideally Solid State Disk (SSD) should be used Network Select the oVirt network to place the OKD cluster hosts on. By default a single network named ovirtmgmt is created at oVirt install IP address for the API endpoint for the OKD cluster The IP address on your network you previously allocated for the OKD cluster API Endpoint IP address for the Ingress for the OKD cluster The IP address on your network you previously allocated for the OKD Cluster Ingress OKD Cluster base domain The base domain your cluster will use for all exposed endpoints - This cannot be changed after installation Cluster Name The name for the OKD cluster, this will be prepended to the base domain to form the URLs for application and API endpoints. (e.g. api.<cluster name>.<base domain> ) Enter a valid Red Hat pull secret This is the pull secret to the Red Hat image registries where Open Shift images are stored. You don't need to use a valid pull secret as OKD does not need access to the registries, but providing a valid secret will allow Red Hat applications to be installed on the cluster. The pull secret must pass a validation check. A valid fake secret is {\"auths\":{\"fake\":{\"auth\":\"aWQ6cGFzcwo=\"}}} . If you have a Red Hat account a valid secret can be downloaded from here Install OKD \u00b6 Complete the following steps to install OKD on your cluster Download the installer \u00b6 To find the latest version of the OKD installer, visit the releases page and download the latest openshift-install image for the platform where you plan to run the installer from (this is typically your local laptop, but could be a virtual machine you run on the ovirt cluster). Expand the downloaded archive to allow access to the openshift-install binary. Create the install config \u00b6 Open a command or terminal window and switch to the directory containing the openshift-install binary run command ./openshift-install create install-config select oVirt as the Platform enter the engine F ully Q ualified D omain N ame accept the suggested engine username (admin@internal) by pressing enter enter the engine password accept the default Cluster name, which should be set as Default unless you have changed oVirt configuration select the appropriate storage domain to store the OKD cluster host disk images select the Network to use input IP address allocated to OKD cluster API endpoint input IP address allocated to OKD cluster Ingress Run the installer \u00b6 Note The default install will download the required content from the internet. It is assumed that the machine running the openshift-install application and the machine hosting oVirt and the OKD cluster have a good internet connection to be able to download the required content. create an install directory mkdir install copy the configuration into the install directory - don't move it, copy it as it gets deleted during the install, so it is good to have a backup copy. cp install-config.yaml install create the cluster with command ./openshift-install create cluster --dir=./install --log-level=info wait until the setup completes, this can take quite a while, 30-60 minutes is not uncommon when the install completes you will see a message similar to this: INFO Install complete! INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/Users/brian/projects/okd/4.7/install/auth/kubeconfig' INFO Access the OpenShift web-console here: https://console-openshift-console.apps.okd.lab.home INFO Login to the console with user: \"kubeadmin\", and password: \"xxxxx-xxxxx-xxxxx-xxxxx\" Connecting to the cluster \u00b6 Web UI \u00b6 Command line \u00b6 Post install tasks \u00b6 Info If you are getting warnings that your cluster is unable to retrieve available updates because https://origin-release.svc.ci.openshift.org host cannot be found, then you have an out of date URL in the update service. You can fix this by running the following command as an admin user on the command line: oc patch clusterversion/version --patch '{\"spec\":{\"upstream\":\"https://amd64.origin.releases.ci.openshift.org/graph\"}}' --type=merge","title":"OKD install"},{"location":"okd_install/#okd-install","text":"When you have your oVirt environment up and running you can install OKD.","title":"OKD Install"},{"location":"okd_install/#preparing-required-information","text":"To help with the installation here is a list of information you will need during the install: item description Engine FQDN This is the Fully Qualified Domain Name of the virtual engine. This is the value you entered in step 1 in oVirt setup section Install the Hosted Engine Engine user This should be admin Engine password This is the value entered at ovirt install time, and the password you use to access the ovirt admin console oVirt Cluster name Should be Default . This can be verified in the oVirt Administration console, select Compute from the side menu then Clusters . By default there is a single cluster defined with Default as the name Storage Domain Select the storage domain with fast storage, ideally Solid State Disk (SSD) should be used Network Select the oVirt network to place the OKD cluster hosts on. By default a single network named ovirtmgmt is created at oVirt install IP address for the API endpoint for the OKD cluster The IP address on your network you previously allocated for the OKD cluster API Endpoint IP address for the Ingress for the OKD cluster The IP address on your network you previously allocated for the OKD Cluster Ingress OKD Cluster base domain The base domain your cluster will use for all exposed endpoints - This cannot be changed after installation Cluster Name The name for the OKD cluster, this will be prepended to the base domain to form the URLs for application and API endpoints. (e.g. api.<cluster name>.<base domain> ) Enter a valid Red Hat pull secret This is the pull secret to the Red Hat image registries where Open Shift images are stored. You don't need to use a valid pull secret as OKD does not need access to the registries, but providing a valid secret will allow Red Hat applications to be installed on the cluster. The pull secret must pass a validation check. A valid fake secret is {\"auths\":{\"fake\":{\"auth\":\"aWQ6cGFzcwo=\"}}} . If you have a Red Hat account a valid secret can be downloaded from here","title":"Preparing required information"},{"location":"okd_install/#install-okd","text":"Complete the following steps to install OKD on your cluster","title":"Install OKD"},{"location":"okd_install/#download-the-installer","text":"To find the latest version of the OKD installer, visit the releases page and download the latest openshift-install image for the platform where you plan to run the installer from (this is typically your local laptop, but could be a virtual machine you run on the ovirt cluster). Expand the downloaded archive to allow access to the openshift-install binary.","title":"Download the installer"},{"location":"okd_install/#create-the-install-config","text":"Open a command or terminal window and switch to the directory containing the openshift-install binary run command ./openshift-install create install-config select oVirt as the Platform enter the engine F ully Q ualified D omain N ame accept the suggested engine username (admin@internal) by pressing enter enter the engine password accept the default Cluster name, which should be set as Default unless you have changed oVirt configuration select the appropriate storage domain to store the OKD cluster host disk images select the Network to use input IP address allocated to OKD cluster API endpoint input IP address allocated to OKD cluster Ingress","title":"Create the install config"},{"location":"okd_install/#run-the-installer","text":"Note The default install will download the required content from the internet. It is assumed that the machine running the openshift-install application and the machine hosting oVirt and the OKD cluster have a good internet connection to be able to download the required content. create an install directory mkdir install copy the configuration into the install directory - don't move it, copy it as it gets deleted during the install, so it is good to have a backup copy. cp install-config.yaml install create the cluster with command ./openshift-install create cluster --dir=./install --log-level=info wait until the setup completes, this can take quite a while, 30-60 minutes is not uncommon when the install completes you will see a message similar to this: INFO Install complete! INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/Users/brian/projects/okd/4.7/install/auth/kubeconfig' INFO Access the OpenShift web-console here: https://console-openshift-console.apps.okd.lab.home INFO Login to the console with user: \"kubeadmin\", and password: \"xxxxx-xxxxx-xxxxx-xxxxx\"","title":"Run the installer"},{"location":"okd_install/#connecting-to-the-cluster","text":"","title":"Connecting to the cluster"},{"location":"okd_install/#web-ui","text":"","title":"Web UI"},{"location":"okd_install/#command-line","text":"","title":"Command line"},{"location":"okd_install/#post-install-tasks","text":"Info If you are getting warnings that your cluster is unable to retrieve available updates because https://origin-release.svc.ci.openshift.org host cannot be found, then you have an out of date URL in the update service. You can fix this by running the following command as an admin user on the command line: oc patch clusterversion/version --patch '{\"spec\":{\"upstream\":\"https://amd64.origin.releases.ci.openshift.org/graph\"}}' --type=merge","title":"Post install tasks"},{"location":"overview/","text":"Overview \u00b6 Getting skills in Cloud Native Development and maintaining those skills can be challenging for a developer. The technology is evolving very quickly, so having an environment where a developer can update, explore and learn new features as they emerge is a valuable resource. Managed cloud based environments are usually a shared resource across a development team, where it is not appropriate to upgrade to the latest emerging technology. as the development environment needs to be a managed, stable environment. Costs can also be an inhibitor to allow developers to have the environments they want to explore and keep up to date, especially when doing so at home. This project documents my cloud native development home lab. I wanted to only use freely available open source technology, but also technology where there is a route into a production environment. Although the project doesn't use any managed cloud resources, it does require a certain amount of hardware - which may not be appropriate for all developers. I purchased a reconditioned workstation with a large amount of memory and Intel Xeon processors, to have sufficient hardware resources to run a cloud environment. This environment will not run on a typical home computer. If you don't have sufficient hardware resources to install a home lab environment, then CodeReady Containers (CRC) may be a solution, as this will run on a laptop or workstation with at least 16GB memory. There are 2 versions of CRC available, the OpenShift version and the version built on top of OKD This project creates a home lab environment based on 2 primary open source projects : oVirt - which is an upstream community run project contributing in to Red Hat Virtualization OKD - which is a community run sibling project to Red Hat OpenShift , taking the same source code as a starting point, then applying some customisations to create the deliverables. oVirt \u00b6 oVirt is an open-source virtualization solution based on the KVM hypervisor. It offers an open-source solution similar to the enterprise products from VMWare, to run a distributed, virtualization solution. In this project, the setup will focus on a home lab deployment, using only a single oVirt host. However, if required larger configurations could be used. OKD \u00b6 OKD is an open-source distribution of Kubernetes, that delivers enhanced capabilities over a standard Kubernetes platform. With additional security features and an integrated user interface with both admin and developer features.","title":"Overview"},{"location":"overview/#overview","text":"Getting skills in Cloud Native Development and maintaining those skills can be challenging for a developer. The technology is evolving very quickly, so having an environment where a developer can update, explore and learn new features as they emerge is a valuable resource. Managed cloud based environments are usually a shared resource across a development team, where it is not appropriate to upgrade to the latest emerging technology. as the development environment needs to be a managed, stable environment. Costs can also be an inhibitor to allow developers to have the environments they want to explore and keep up to date, especially when doing so at home. This project documents my cloud native development home lab. I wanted to only use freely available open source technology, but also technology where there is a route into a production environment. Although the project doesn't use any managed cloud resources, it does require a certain amount of hardware - which may not be appropriate for all developers. I purchased a reconditioned workstation with a large amount of memory and Intel Xeon processors, to have sufficient hardware resources to run a cloud environment. This environment will not run on a typical home computer. If you don't have sufficient hardware resources to install a home lab environment, then CodeReady Containers (CRC) may be a solution, as this will run on a laptop or workstation with at least 16GB memory. There are 2 versions of CRC available, the OpenShift version and the version built on top of OKD This project creates a home lab environment based on 2 primary open source projects : oVirt - which is an upstream community run project contributing in to Red Hat Virtualization OKD - which is a community run sibling project to Red Hat OpenShift , taking the same source code as a starting point, then applying some customisations to create the deliverables.","title":"Overview"},{"location":"overview/#ovirt","text":"oVirt is an open-source virtualization solution based on the KVM hypervisor. It offers an open-source solution similar to the enterprise products from VMWare, to run a distributed, virtualization solution. In this project, the setup will focus on a home lab deployment, using only a single oVirt host. However, if required larger configurations could be used.","title":"oVirt"},{"location":"overview/#okd","text":"OKD is an open-source distribution of Kubernetes, that delivers enhanced capabilities over a standard Kubernetes platform. With additional security features and an integrated user interface with both admin and developer features.","title":"OKD"},{"location":"ovirt/","text":"oVirt installation and setup \u00b6 Install oVirt node \u00b6 download ISO flash to USB memory stick Boot from memory stick Run oVirt setup wizard \u00b6 Select language from the list then press the Continue button Select the keyboard layout then press the Done key Enter the Installation Destination section you need to keep a disk or partition free for Gluster to use (this was discussed in the preparation section) Enter a root password Set the TimeZone Enter the Network and Host name section select Configure to configure the network interface In the IPv4 tab select Manual configuration then select Add to enter the configuration - this should match what is in your DNS configuration. Press Save to store the IP configuration Click the toggle switch to enable the Ethernet interface Select Done to complete the network configuration (optional) If you want to configure NTP time synchronisation, revisit the Time & Date section and configure the NTP server to a local server then enable NTP Select Begin Installation to start the installation Wait for installation to complete then select to reboot when selected (ensure the USB memory stick is removed so it doesn't boot from the install media) You can now interact with the installed system using the web based cockpit UI. On your laptop or a workstation on the network, navigate to https://<host address>:9090 . Where the host address is the address you added in DNS for the IP address you manually configured in step 6 above. (Optional) Create partition for Gluster filesystem \u00b6 If you are using a disk partition for the Gluster FS, then you need to create the partition. In the cockpit UI, navigate to the Storage section. Select the disk where the partition will live from the side panel. This will show the partitions in the Content section. Next to the unallocated space there is a Create partition button. Click the button to create a partition. Set the required size and select No filesystem as the type then click Create partition to create the partition. Setup ssh keys \u00b6 During the install the ansible scripts need to be able to execute commands on the host. To do this it uses passwordless ssh. Even though we are only using a single host, the scripts are written to work across multiple hosts, so we need to enable passwordless ssh. You need to know your host IP address to substitute into ssh-copy-id command. You can find your host address using command hostname -I Enter the following commands using the terminal section of the cockpit web console ( http://<host-address>:9090 ) for the oVirt host. Substitute your host address in the copy command, accept all defaults and leave the passphrase blank: ssh-keygen ssh-copy-id root@<host-address> you will be prompted to confirm the connection, answer yes then you will be prompted for the root password, which you entered in step 4 of the previous section. Configure LVM filter \u00b6 By default Logical Volumes are configured to specific devices, so you need to add the device you want to use for the Gluster storage. Using the terminal section in the cockpit web ui, edit file /etc/lvm/lvm.conf and search for a line the starts filter = , not lines starting with # are comments. Modify the line to include your chosen device for the gluster volumes, eg. If the filter is currently set to filter = [\"a|^/dev/disk/by-id/lvm-pv-uuid-U77HZ9-LPry-OOMY-bgOq-t34w-l3av-srg8tU$|\", \"r|.*|\"] and your chosen device is /dev/sdb , then the filter line needs to be modified to: filter = [\"a|^/dev/disk/by-id/lvm-pv-uuid-U77HZ9-LPry-OOMY-bgOq-t34w-l3av-srg8tU$|\", \"a|^/dev/sdb|\", \"r|.*|\"] Save the modified lvm.conf file Note If you plan to use an entire disk for GlusterFS, then it is important that the disk is not partitioned, so if it has previously been used and has a partition table on it, then use the Terminal section of the cockpit interface to clear the device. E.g. if you will be using the disk /dev/sdb for gluster, then wipe the disk using command wipefs -a -f /dev/sdb . This will erase the disk. Setup the hyperconverged oVirt Hosted Engine and Gluster storage \u00b6 The hyperconverged Hosted Engine and Gluster storage can be installed using the cockpit web console ( http://<host-address>:9090 ). In the Cockpit UI navigate to the Virtualization section in the side menu, then select the Hosted Engine section. Select the start button under Hyperconverged. Select the Run Gluster Wizard for Single Node option to start the install. Enter the Fully qualified hostname for the oVirt cluster (this should be the name of the oVirt node that was used during the ssh-copy-id command) You can leave the Packages options as default You can leave the Volumes options as default. However, if you have a small amount of storage you may want to delete 2 of the 3 default volumes, so all available storage will be in a single volume. In the Bricks options, set the device to the device you have available for the gluster storage for all the configured volumes. This could be an entire disk (e.g. /dev/sdb), or an available disk partition (/dev/sda3) In the review section edit the summary and remove the line - 5900/tcp , as this will cause the scripts to fail. Save the script Select the Deploy button to start the Gluster Storage installation If all works you should see the screen confirming the Gluster installation was successful You can now select the Continue to Hosted Engine Deployment button to install the Hosted Engine. Install Hosted Engine \u00b6 The Hosted Engine will try to add port 6900 to the public zone and will fail as the port is already being exposed. Before installing the Hosted Engine you need to modify the firewall configuration. Modify the firewall rules \u00b6 You need to modify the firewall config to remove port 6900 to ensure the automated deployment will work. Switch to the terminal section and edit the file /etc/firewalld/zones/public.xml to have the following content: <zone> <short> Public </short> <description> For use in public areas. You do not trust the other computers on networks to not harm your computer. Only selected incoming connections are accepted. </description> <service name= \"ssh\" /> <service name= \"dhcpv6-client\" /> <service name= \"cockpit\" /> <service name= \"libvirt\" /> <service name= \"libvirt-tls\" /> <service name= \"glusterfs\" /> <service name= \"custom-vdsm\" /> <port port= \"2049\" protocol= \"tcp\" /> <port port= \"54321\" protocol= \"tcp\" /> <port port= \"5900-6899\" protocol= \"tcp\" /> <port port= \"6901-6923\" protocol= \"tcp\" /> <port port= \"5666\" protocol= \"tcp\" /> <port port= \"16514\" protocol= \"tcp\" /> </zone> You can see the port range 5900-6923 has been split into 2 ranges avoiding port 6900 run command systemctl restart firewalld to make the new firewall configuration live. You can now return to the Virtualization section of the Cockpit web console to continue with the Hosted Engine deployment. Install the Hosted Engine \u00b6 Select to Deploy the Hosted Engine In the VM Settings section enter: the fully qualified host name for the hosted engine switch to static Network Configuration and set the IP address to the value matching your hostname in your DNS, the gateway address and DNS server address enter the root password you want to set for the hosted engine operating system select Next to move to Engine section In the Engine section enter the admin password you want to set for the Engine, then select **Next to move to the next section Review the configuration information then press Prepare VM to deploy the Hosted Engine virtual machine in the Prepare VM section, this can take a while When the VM has been setup press Next to complete the setup In the storage section the settings should be pre-populated, so you can press Next then Finish Deployment","title":"oVirt setup"},{"location":"ovirt/#ovirt-installation-and-setup","text":"","title":"oVirt installation and setup"},{"location":"ovirt/#install-ovirt-node","text":"download ISO flash to USB memory stick Boot from memory stick","title":"Install oVirt node"},{"location":"ovirt/#run-ovirt-setup-wizard","text":"Select language from the list then press the Continue button Select the keyboard layout then press the Done key Enter the Installation Destination section you need to keep a disk or partition free for Gluster to use (this was discussed in the preparation section) Enter a root password Set the TimeZone Enter the Network and Host name section select Configure to configure the network interface In the IPv4 tab select Manual configuration then select Add to enter the configuration - this should match what is in your DNS configuration. Press Save to store the IP configuration Click the toggle switch to enable the Ethernet interface Select Done to complete the network configuration (optional) If you want to configure NTP time synchronisation, revisit the Time & Date section and configure the NTP server to a local server then enable NTP Select Begin Installation to start the installation Wait for installation to complete then select to reboot when selected (ensure the USB memory stick is removed so it doesn't boot from the install media) You can now interact with the installed system using the web based cockpit UI. On your laptop or a workstation on the network, navigate to https://<host address>:9090 . Where the host address is the address you added in DNS for the IP address you manually configured in step 6 above.","title":"Run oVirt setup wizard"},{"location":"ovirt/#optional-create-partition-for-gluster-filesystem","text":"If you are using a disk partition for the Gluster FS, then you need to create the partition. In the cockpit UI, navigate to the Storage section. Select the disk where the partition will live from the side panel. This will show the partitions in the Content section. Next to the unallocated space there is a Create partition button. Click the button to create a partition. Set the required size and select No filesystem as the type then click Create partition to create the partition.","title":"(Optional) Create partition for Gluster filesystem"},{"location":"ovirt/#setup-ssh-keys","text":"During the install the ansible scripts need to be able to execute commands on the host. To do this it uses passwordless ssh. Even though we are only using a single host, the scripts are written to work across multiple hosts, so we need to enable passwordless ssh. You need to know your host IP address to substitute into ssh-copy-id command. You can find your host address using command hostname -I Enter the following commands using the terminal section of the cockpit web console ( http://<host-address>:9090 ) for the oVirt host. Substitute your host address in the copy command, accept all defaults and leave the passphrase blank: ssh-keygen ssh-copy-id root@<host-address> you will be prompted to confirm the connection, answer yes then you will be prompted for the root password, which you entered in step 4 of the previous section.","title":"Setup ssh keys"},{"location":"ovirt/#configure-lvm-filter","text":"By default Logical Volumes are configured to specific devices, so you need to add the device you want to use for the Gluster storage. Using the terminal section in the cockpit web ui, edit file /etc/lvm/lvm.conf and search for a line the starts filter = , not lines starting with # are comments. Modify the line to include your chosen device for the gluster volumes, eg. If the filter is currently set to filter = [\"a|^/dev/disk/by-id/lvm-pv-uuid-U77HZ9-LPry-OOMY-bgOq-t34w-l3av-srg8tU$|\", \"r|.*|\"] and your chosen device is /dev/sdb , then the filter line needs to be modified to: filter = [\"a|^/dev/disk/by-id/lvm-pv-uuid-U77HZ9-LPry-OOMY-bgOq-t34w-l3av-srg8tU$|\", \"a|^/dev/sdb|\", \"r|.*|\"] Save the modified lvm.conf file Note If you plan to use an entire disk for GlusterFS, then it is important that the disk is not partitioned, so if it has previously been used and has a partition table on it, then use the Terminal section of the cockpit interface to clear the device. E.g. if you will be using the disk /dev/sdb for gluster, then wipe the disk using command wipefs -a -f /dev/sdb . This will erase the disk.","title":"Configure LVM filter"},{"location":"ovirt/#setup-the-hyperconverged-ovirt-hosted-engine-and-gluster-storage","text":"The hyperconverged Hosted Engine and Gluster storage can be installed using the cockpit web console ( http://<host-address>:9090 ). In the Cockpit UI navigate to the Virtualization section in the side menu, then select the Hosted Engine section. Select the start button under Hyperconverged. Select the Run Gluster Wizard for Single Node option to start the install. Enter the Fully qualified hostname for the oVirt cluster (this should be the name of the oVirt node that was used during the ssh-copy-id command) You can leave the Packages options as default You can leave the Volumes options as default. However, if you have a small amount of storage you may want to delete 2 of the 3 default volumes, so all available storage will be in a single volume. In the Bricks options, set the device to the device you have available for the gluster storage for all the configured volumes. This could be an entire disk (e.g. /dev/sdb), or an available disk partition (/dev/sda3) In the review section edit the summary and remove the line - 5900/tcp , as this will cause the scripts to fail. Save the script Select the Deploy button to start the Gluster Storage installation If all works you should see the screen confirming the Gluster installation was successful You can now select the Continue to Hosted Engine Deployment button to install the Hosted Engine.","title":"Setup the hyperconverged oVirt Hosted Engine and Gluster storage"},{"location":"ovirt/#install-hosted-engine","text":"The Hosted Engine will try to add port 6900 to the public zone and will fail as the port is already being exposed. Before installing the Hosted Engine you need to modify the firewall configuration.","title":"Install Hosted Engine"},{"location":"ovirt/#modify-the-firewall-rules","text":"You need to modify the firewall config to remove port 6900 to ensure the automated deployment will work. Switch to the terminal section and edit the file /etc/firewalld/zones/public.xml to have the following content: <zone> <short> Public </short> <description> For use in public areas. You do not trust the other computers on networks to not harm your computer. Only selected incoming connections are accepted. </description> <service name= \"ssh\" /> <service name= \"dhcpv6-client\" /> <service name= \"cockpit\" /> <service name= \"libvirt\" /> <service name= \"libvirt-tls\" /> <service name= \"glusterfs\" /> <service name= \"custom-vdsm\" /> <port port= \"2049\" protocol= \"tcp\" /> <port port= \"54321\" protocol= \"tcp\" /> <port port= \"5900-6899\" protocol= \"tcp\" /> <port port= \"6901-6923\" protocol= \"tcp\" /> <port port= \"5666\" protocol= \"tcp\" /> <port port= \"16514\" protocol= \"tcp\" /> </zone> You can see the port range 5900-6923 has been split into 2 ranges avoiding port 6900 run command systemctl restart firewalld to make the new firewall configuration live. You can now return to the Virtualization section of the Cockpit web console to continue with the Hosted Engine deployment.","title":"Modify the firewall rules"},{"location":"ovirt/#install-the-hosted-engine","text":"Select to Deploy the Hosted Engine In the VM Settings section enter: the fully qualified host name for the hosted engine switch to static Network Configuration and set the IP address to the value matching your hostname in your DNS, the gateway address and DNS server address enter the root password you want to set for the hosted engine operating system select Next to move to Engine section In the Engine section enter the admin password you want to set for the Engine, then select **Next to move to the next section Review the configuration information then press Prepare VM to deploy the Hosted Engine virtual machine in the Prepare VM section, this can take a while When the VM has been setup press Next to complete the setup In the storage section the settings should be pre-populated, so you can press Next then Finish Deployment","title":"Install the Hosted Engine"},{"location":"preparation/","text":"Getting ready for the installation \u00b6 Before starting to install oVirt and OKD you need to ensure you have the necessary environment available. This includes: Having sufficient compute, memory and storage available for the cluster Having the required networking infrastructure and services available and configured Hardware requirements \u00b6 The OKD documentation describes an OKD production environment on oVirt, which specifies hardware requirements of: Minimum 28 vCPUs 112 GiB RAM min 230 GiB storage (non-prod) or 840 GiB (prod) The documentation also assumes that oVirt will be installed over multiple physical hosts, to provide a more resilient environment. However, for this home lab setup a single machine will be used. Todo Checkout the memory requirements mentioned below However, for a Home Lab environment the oVirt environment can be configured to over commit memory, to allow a cluster to be setup with a minimum of 56 GiB memory, but then you will be constrained on what you can deploy into the cluster. There is also the option to not deploy a fault tolerant OKD cluster and reduce the number of Master nodes, however, if going down that route CodeReady Containers may be a better option. Storage setup \u00b6 Storage partition for Gluster split disk usage Network Requirements \u00b6 IP Addresses and DNS resolution needed oVirt host virtual Engine OKD","title":"Preparing"},{"location":"preparation/#getting-ready-for-the-installation","text":"Before starting to install oVirt and OKD you need to ensure you have the necessary environment available. This includes: Having sufficient compute, memory and storage available for the cluster Having the required networking infrastructure and services available and configured","title":"Getting ready for the installation"},{"location":"preparation/#hardware-requirements","text":"The OKD documentation describes an OKD production environment on oVirt, which specifies hardware requirements of: Minimum 28 vCPUs 112 GiB RAM min 230 GiB storage (non-prod) or 840 GiB (prod) The documentation also assumes that oVirt will be installed over multiple physical hosts, to provide a more resilient environment. However, for this home lab setup a single machine will be used. Todo Checkout the memory requirements mentioned below However, for a Home Lab environment the oVirt environment can be configured to over commit memory, to allow a cluster to be setup with a minimum of 56 GiB memory, but then you will be constrained on what you can deploy into the cluster. There is also the option to not deploy a fault tolerant OKD cluster and reduce the number of Master nodes, however, if going down that route CodeReady Containers may be a better option.","title":"Hardware requirements"},{"location":"preparation/#storage-setup","text":"Storage partition for Gluster split disk usage","title":"Storage setup"},{"location":"preparation/#network-requirements","text":"IP Addresses and DNS resolution needed oVirt host virtual Engine OKD","title":"Network Requirements"},{"location":"resources/","text":"Additional Resources \u00b6 Below are some links and posts about OKD. They may provide additional insight and options to help your OKD learning. Community locations \u00b6 OKD main site OKD git repo OKD git community repo Installation and setup help \u00b6 Craig Robinson - Installing an OKD 4.5 Cluster Ross Brigoli - Running OpenShift at Home Josphat Mutal - How to Setup Local OpenShift Origin (OKD) Cluster on CentOS 7 Oren Oichman - Deploy OKD in a disconnected (Air Gap) Environment Josphat Mutal - How to install OKD OpenShift 4.7 Cluster on OpenStack OKD 4 additional content \u00b6 OKD 4 Testing and Deployment Workshop - Videos and Additional Resources OKD 4 blogs on OpenShift Blog site OKD blogs site","title":"Resources"},{"location":"resources/#additional-resources","text":"Below are some links and posts about OKD. They may provide additional insight and options to help your OKD learning.","title":"Additional Resources"},{"location":"resources/#community-locations","text":"OKD main site OKD git repo OKD git community repo","title":"Community locations"},{"location":"resources/#installation-and-setup-help","text":"Craig Robinson - Installing an OKD 4.5 Cluster Ross Brigoli - Running OpenShift at Home Josphat Mutal - How to Setup Local OpenShift Origin (OKD) Cluster on CentOS 7 Oren Oichman - Deploy OKD in a disconnected (Air Gap) Environment Josphat Mutal - How to install OKD OpenShift 4.7 Cluster on OpenStack","title":"Installation and setup help"},{"location":"resources/#okd-4-additional-content","text":"OKD 4 Testing and Deployment Workshop - Videos and Additional Resources OKD 4 blogs on OpenShift Blog site OKD blogs site","title":"OKD 4 additional content"}]}